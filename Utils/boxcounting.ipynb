{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/a/akshatha.mohan/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/grads/a/akshatha.mohan/anaconda3/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import manifold\n",
    "from scipy import stats\n",
    "from PIL import Image\n",
    "import pdb\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 9., 11.,  8., 20.],\n",
       "          [18., 18., 17.,  3.],\n",
       "          [10.,  7., 18., 16.],\n",
       "          [18., 17., 22., 13.]]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "image = torch.tensor([[9, 11, 8, 20], [18, 18, 17, 3], [10, 7, 18, 16], [18, 17, 22, 13]], dtype=torch.float32)\n",
    "image = image.unsqueeze(0).unsqueeze(0)\n",
    "image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0139]],\n",
      "\n",
      "         [[0.0169]],\n",
      "\n",
      "         [[0.0132]]],\n",
      "\n",
      "\n",
      "        [[[0.0145]],\n",
      "\n",
      "         [[0.0128]],\n",
      "\n",
      "         [[0.0130]]],\n",
      "\n",
      "\n",
      "        [[[0.0123]],\n",
      "\n",
      "         [[0.0130]],\n",
      "\n",
      "         [[0.0172]]],\n",
      "\n",
      "\n",
      "        [[[0.0167]],\n",
      "\n",
      "         [[0.0137]],\n",
      "\n",
      "         [[0.0169]]],\n",
      "\n",
      "\n",
      "        [[[0.0127]],\n",
      "\n",
      "         [[0.0147]],\n",
      "\n",
      "         [[0.0135]]],\n",
      "\n",
      "\n",
      "        [[[0.0133]],\n",
      "\n",
      "         [[0.0152]],\n",
      "\n",
      "         [[0.0132]]],\n",
      "\n",
      "\n",
      "        [[[0.0130]],\n",
      "\n",
      "         [[0.0143]],\n",
      "\n",
      "         [[0.0130]]],\n",
      "\n",
      "\n",
      "        [[[0.0161]],\n",
      "\n",
      "         [[0.0147]],\n",
      "\n",
      "         [[0.0147]]],\n",
      "\n",
      "\n",
      "        [[[0.0143]],\n",
      "\n",
      "         [[0.0227]],\n",
      "\n",
      "         [[0.0149]]],\n",
      "\n",
      "\n",
      "        [[[0.0149]],\n",
      "\n",
      "         [[0.0133]],\n",
      "\n",
      "         [[0.0169]]],\n",
      "\n",
      "\n",
      "        [[[0.0145]],\n",
      "\n",
      "         [[0.0137]],\n",
      "\n",
      "         [[0.0137]]],\n",
      "\n",
      "\n",
      "        [[[0.0132]],\n",
      "\n",
      "         [[0.0125]],\n",
      "\n",
      "         [[0.0123]]],\n",
      "\n",
      "\n",
      "        [[[0.0167]],\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         [[0.0130]]],\n",
      "\n",
      "\n",
      "        [[[0.0172]],\n",
      "\n",
      "         [[0.0185]],\n",
      "\n",
      "         [[0.0145]]],\n",
      "\n",
      "\n",
      "        [[[0.0167]],\n",
      "\n",
      "         [[0.0175]],\n",
      "\n",
      "         [[0.0164]]],\n",
      "\n",
      "\n",
      "        [[[0.0208]],\n",
      "\n",
      "         [[0.0128]],\n",
      "\n",
      "         [[0.0130]]],\n",
      "\n",
      "\n",
      "        [[[0.0132]],\n",
      "\n",
      "         [[0.0132]],\n",
      "\n",
      "         [[0.0149]]],\n",
      "\n",
      "\n",
      "        [[[0.0192]],\n",
      "\n",
      "         [[0.0185]],\n",
      "\n",
      "         [[0.0159]]],\n",
      "\n",
      "\n",
      "        [[[0.0185]],\n",
      "\n",
      "         [[0.0123]],\n",
      "\n",
      "         [[0.0132]]],\n",
      "\n",
      "\n",
      "        [[[0.0145]],\n",
      "\n",
      "         [[0.0137]],\n",
      "\n",
      "         [[0.0125]]],\n",
      "\n",
      "\n",
      "        [[[0.0167]],\n",
      "\n",
      "         [[0.0164]],\n",
      "\n",
      "         [[0.0128]]],\n",
      "\n",
      "\n",
      "        [[[0.0135]],\n",
      "\n",
      "         [[0.0139]],\n",
      "\n",
      "         [[0.0132]]],\n",
      "\n",
      "\n",
      "        [[[0.0145]],\n",
      "\n",
      "         [[0.0127]],\n",
      "\n",
      "         [[0.0130]]],\n",
      "\n",
      "\n",
      "        [[[0.0213]],\n",
      "\n",
      "         [[0.0143]],\n",
      "\n",
      "         [[0.0123]]],\n",
      "\n",
      "\n",
      "        [[[0.0143]],\n",
      "\n",
      "         [[0.0137]],\n",
      "\n",
      "         [[0.0147]]],\n",
      "\n",
      "\n",
      "        [[[0.0123]],\n",
      "\n",
      "         [[0.0154]],\n",
      "\n",
      "         [[0.0164]]],\n",
      "\n",
      "\n",
      "        [[[0.0164]],\n",
      "\n",
      "         [[0.0185]],\n",
      "\n",
      "         [[0.0128]]],\n",
      "\n",
      "\n",
      "        [[[0.0139]],\n",
      "\n",
      "         [[0.0130]],\n",
      "\n",
      "         [[0.0154]]],\n",
      "\n",
      "\n",
      "        [[[0.0139]],\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         [[0.0130]]],\n",
      "\n",
      "\n",
      "        [[[0.0147]],\n",
      "\n",
      "         [[0.0143]],\n",
      "\n",
      "         [[0.0135]]],\n",
      "\n",
      "\n",
      "        [[[0.0145]],\n",
      "\n",
      "         [[0.0128]],\n",
      "\n",
      "         [[0.0161]]],\n",
      "\n",
      "\n",
      "        [[[0.0192]],\n",
      "\n",
      "         [[0.0172]],\n",
      "\n",
      "         [[0.0133]]],\n",
      "\n",
      "\n",
      "        [[[0.0147]],\n",
      "\n",
      "         [[0.0123]],\n",
      "\n",
      "         [[0.0143]]],\n",
      "\n",
      "\n",
      "        [[[0.0130]],\n",
      "\n",
      "         [[0.0167]],\n",
      "\n",
      "         [[0.0130]]],\n",
      "\n",
      "\n",
      "        [[[0.0133]],\n",
      "\n",
      "         [[0.0143]],\n",
      "\n",
      "         [[0.0192]]],\n",
      "\n",
      "\n",
      "        [[[0.0125]],\n",
      "\n",
      "         [[0.0127]],\n",
      "\n",
      "         [[0.0125]]],\n",
      "\n",
      "\n",
      "        [[[0.0145]],\n",
      "\n",
      "         [[0.0159]],\n",
      "\n",
      "         [[0.0147]]],\n",
      "\n",
      "\n",
      "        [[[0.0147]],\n",
      "\n",
      "         [[0.0125]],\n",
      "\n",
      "         [[0.0161]]],\n",
      "\n",
      "\n",
      "        [[[0.0145]],\n",
      "\n",
      "         [[0.0137]],\n",
      "\n",
      "         [[0.0156]]],\n",
      "\n",
      "\n",
      "        [[[0.0141]],\n",
      "\n",
      "         [[0.0125]],\n",
      "\n",
      "         [[0.0128]]],\n",
      "\n",
      "\n",
      "        [[[0.0141]],\n",
      "\n",
      "         [[0.0133]],\n",
      "\n",
      "         [[0.0127]]],\n",
      "\n",
      "\n",
      "        [[[0.0122]],\n",
      "\n",
      "         [[0.0182]],\n",
      "\n",
      "         [[0.0156]]],\n",
      "\n",
      "\n",
      "        [[[0.0123]],\n",
      "\n",
      "         [[0.0130]],\n",
      "\n",
      "         [[0.0147]]],\n",
      "\n",
      "\n",
      "        [[[0.0143]],\n",
      "\n",
      "         [[0.0132]],\n",
      "\n",
      "         [[0.0130]]],\n",
      "\n",
      "\n",
      "        [[[0.0200]],\n",
      "\n",
      "         [[0.0159]],\n",
      "\n",
      "         [[0.0185]]],\n",
      "\n",
      "\n",
      "        [[[0.0128]],\n",
      "\n",
      "         [[0.0152]],\n",
      "\n",
      "         [[0.0164]]],\n",
      "\n",
      "\n",
      "        [[[0.0172]],\n",
      "\n",
      "         [[0.0135]],\n",
      "\n",
      "         [[0.0152]]],\n",
      "\n",
      "\n",
      "        [[[0.0132]],\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         [[0.0179]]],\n",
      "\n",
      "\n",
      "        [[[0.0154]],\n",
      "\n",
      "         [[0.0192]],\n",
      "\n",
      "         [[0.0135]]],\n",
      "\n",
      "\n",
      "        [[[0.0147]],\n",
      "\n",
      "         [[0.0143]],\n",
      "\n",
      "         [[0.0179]]],\n",
      "\n",
      "\n",
      "        [[[0.0130]],\n",
      "\n",
      "         [[0.0161]],\n",
      "\n",
      "         [[0.0159]]],\n",
      "\n",
      "\n",
      "        [[[0.0169]],\n",
      "\n",
      "         [[0.0278]],\n",
      "\n",
      "         [[0.0179]]],\n",
      "\n",
      "\n",
      "        [[[0.0139]],\n",
      "\n",
      "         [[0.0139]],\n",
      "\n",
      "         [[0.0139]]],\n",
      "\n",
      "\n",
      "        [[[0.0164]],\n",
      "\n",
      "         [[0.0135]],\n",
      "\n",
      "         [[0.0196]]],\n",
      "\n",
      "\n",
      "        [[[0.0145]],\n",
      "\n",
      "         [[0.0128]],\n",
      "\n",
      "         [[0.0161]]],\n",
      "\n",
      "\n",
      "        [[[0.0143]],\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         [[0.0130]]],\n",
      "\n",
      "\n",
      "        [[[0.0141]],\n",
      "\n",
      "         [[0.0159]],\n",
      "\n",
      "         [[0.0161]]],\n",
      "\n",
      "\n",
      "        [[[0.0128]],\n",
      "\n",
      "         [[0.0147]],\n",
      "\n",
      "         [[0.0167]]],\n",
      "\n",
      "\n",
      "        [[[0.0156]],\n",
      "\n",
      "         [[0.0167]],\n",
      "\n",
      "         [[0.0169]]],\n",
      "\n",
      "\n",
      "        [[[0.0139]],\n",
      "\n",
      "         [[0.0143]],\n",
      "\n",
      "         [[0.0200]]],\n",
      "\n",
      "\n",
      "        [[[0.0185]],\n",
      "\n",
      "         [[0.0133]],\n",
      "\n",
      "         [[0.0154]]],\n",
      "\n",
      "\n",
      "        [[[0.0130]],\n",
      "\n",
      "         [[0.0200]],\n",
      "\n",
      "         [[0.0147]]],\n",
      "\n",
      "\n",
      "        [[[0.0164]],\n",
      "\n",
      "         [[0.0130]],\n",
      "\n",
      "         [[0.0130]]],\n",
      "\n",
      "\n",
      "        [[[0.0145]],\n",
      "\n",
      "         [[0.0192]],\n",
      "\n",
      "         [[0.0127]]],\n",
      "\n",
      "\n",
      "        [[[0.0123]],\n",
      "\n",
      "         [[0.0141]],\n",
      "\n",
      "         [[0.0204]]],\n",
      "\n",
      "\n",
      "        [[[0.0233]],\n",
      "\n",
      "         [[0.0182]],\n",
      "\n",
      "         [[0.0137]]],\n",
      "\n",
      "\n",
      "        [[[0.0123]],\n",
      "\n",
      "         [[0.0159]],\n",
      "\n",
      "         [[0.0161]]],\n",
      "\n",
      "\n",
      "        [[[0.0128]],\n",
      "\n",
      "         [[0.0139]],\n",
      "\n",
      "         [[0.0147]]],\n",
      "\n",
      "\n",
      "        [[[0.0164]],\n",
      "\n",
      "         [[0.0161]],\n",
      "\n",
      "         [[0.0128]]],\n",
      "\n",
      "\n",
      "        [[[0.0135]],\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         [[0.0152]]],\n",
      "\n",
      "\n",
      "        [[[0.0143]],\n",
      "\n",
      "         [[0.0133]],\n",
      "\n",
      "         [[0.0154]]],\n",
      "\n",
      "\n",
      "        [[[0.0141]],\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         [[0.0137]]],\n",
      "\n",
      "\n",
      "        [[[0.0161]],\n",
      "\n",
      "         [[0.0167]],\n",
      "\n",
      "         [[0.0175]]],\n",
      "\n",
      "\n",
      "        [[[0.0122]],\n",
      "\n",
      "         [[0.0149]],\n",
      "\n",
      "         [[0.0130]]],\n",
      "\n",
      "\n",
      "        [[[0.0143]],\n",
      "\n",
      "         [[0.0154]],\n",
      "\n",
      "         [[0.0139]]],\n",
      "\n",
      "\n",
      "        [[[0.0222]],\n",
      "\n",
      "         [[0.0179]],\n",
      "\n",
      "         [[0.0213]]],\n",
      "\n",
      "\n",
      "        [[[0.0130]],\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         [[0.0179]]],\n",
      "\n",
      "\n",
      "        [[[0.0159]],\n",
      "\n",
      "         [[0.0167]],\n",
      "\n",
      "         [[0.0132]]],\n",
      "\n",
      "\n",
      "        [[[0.0133]],\n",
      "\n",
      "         [[0.0167]],\n",
      "\n",
      "         [[0.0139]]],\n",
      "\n",
      "\n",
      "        [[[0.0185]],\n",
      "\n",
      "         [[0.0141]],\n",
      "\n",
      "         [[0.0164]]],\n",
      "\n",
      "\n",
      "        [[[0.0159]],\n",
      "\n",
      "         [[0.0169]],\n",
      "\n",
      "         [[0.0286]]],\n",
      "\n",
      "\n",
      "        [[[0.0139]],\n",
      "\n",
      "         [[0.0172]],\n",
      "\n",
      "         [[0.0149]]],\n",
      "\n",
      "\n",
      "        [[[0.0152]],\n",
      "\n",
      "         [[0.0154]],\n",
      "\n",
      "         [[0.0145]]],\n",
      "\n",
      "\n",
      "        [[[0.0122]],\n",
      "\n",
      "         [[0.0182]],\n",
      "\n",
      "         [[0.0133]]],\n",
      "\n",
      "\n",
      "        [[[0.0152]],\n",
      "\n",
      "         [[0.0125]],\n",
      "\n",
      "         [[0.0147]]],\n",
      "\n",
      "\n",
      "        [[[0.0164]],\n",
      "\n",
      "         [[0.0196]],\n",
      "\n",
      "         [[0.0159]]],\n",
      "\n",
      "\n",
      "        [[[0.0123]],\n",
      "\n",
      "         [[0.0147]],\n",
      "\n",
      "         [[0.0137]]],\n",
      "\n",
      "\n",
      "        [[[0.0130]],\n",
      "\n",
      "         [[0.0152]],\n",
      "\n",
      "         [[0.0141]]],\n",
      "\n",
      "\n",
      "        [[[0.0156]],\n",
      "\n",
      "         [[0.0130]],\n",
      "\n",
      "         [[0.0125]]],\n",
      "\n",
      "\n",
      "        [[[0.0179]],\n",
      "\n",
      "         [[0.0133]],\n",
      "\n",
      "         [[0.0179]]],\n",
      "\n",
      "\n",
      "        [[[0.0175]],\n",
      "\n",
      "         [[0.0139]],\n",
      "\n",
      "         [[0.0156]]],\n",
      "\n",
      "\n",
      "        [[[0.0222]],\n",
      "\n",
      "         [[0.0130]],\n",
      "\n",
      "         [[0.0145]]],\n",
      "\n",
      "\n",
      "        [[[0.0128]],\n",
      "\n",
      "         [[0.0122]],\n",
      "\n",
      "         [[0.0128]]],\n",
      "\n",
      "\n",
      "        [[[0.0143]],\n",
      "\n",
      "         [[0.0233]],\n",
      "\n",
      "         [[0.0123]]],\n",
      "\n",
      "\n",
      "        [[[0.0159]],\n",
      "\n",
      "         [[0.0294]],\n",
      "\n",
      "         [[0.0169]]],\n",
      "\n",
      "\n",
      "        [[[0.0133]],\n",
      "\n",
      "         [[0.0175]],\n",
      "\n",
      "         [[0.0137]]],\n",
      "\n",
      "\n",
      "        [[[0.0185]],\n",
      "\n",
      "         [[0.0135]],\n",
      "\n",
      "         [[0.0200]]],\n",
      "\n",
      "\n",
      "        [[[0.0132]],\n",
      "\n",
      "         [[0.0130]],\n",
      "\n",
      "         [[0.0149]]],\n",
      "\n",
      "\n",
      "        [[[0.0256]],\n",
      "\n",
      "         [[0.0127]],\n",
      "\n",
      "         [[0.0145]]],\n",
      "\n",
      "\n",
      "        [[[0.0132]],\n",
      "\n",
      "         [[0.0123]],\n",
      "\n",
      "         [[0.0145]]],\n",
      "\n",
      "\n",
      "        [[[0.0175]],\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         [[0.0137]]],\n",
      "\n",
      "\n",
      "        [[[0.0189]],\n",
      "\n",
      "         [[0.0172]],\n",
      "\n",
      "         [[0.0147]]],\n",
      "\n",
      "\n",
      "        [[[0.0149]],\n",
      "\n",
      "         [[0.0312]],\n",
      "\n",
      "         [[0.0175]]],\n",
      "\n",
      "\n",
      "        [[[0.0250]],\n",
      "\n",
      "         [[0.0156]],\n",
      "\n",
      "         [[0.0135]]],\n",
      "\n",
      "\n",
      "        [[[0.0125]],\n",
      "\n",
      "         [[0.0175]],\n",
      "\n",
      "         [[0.0143]]],\n",
      "\n",
      "\n",
      "        [[[0.0164]],\n",
      "\n",
      "         [[0.0141]],\n",
      "\n",
      "         [[0.0175]]],\n",
      "\n",
      "\n",
      "        [[[0.0147]],\n",
      "\n",
      "         [[0.0143]],\n",
      "\n",
      "         [[0.0185]]],\n",
      "\n",
      "\n",
      "        [[[0.0172]],\n",
      "\n",
      "         [[0.0133]],\n",
      "\n",
      "         [[0.0149]]],\n",
      "\n",
      "\n",
      "        [[[0.0145]],\n",
      "\n",
      "         [[0.0169]],\n",
      "\n",
      "         [[0.0139]]],\n",
      "\n",
      "\n",
      "        [[[0.0143]],\n",
      "\n",
      "         [[0.0154]],\n",
      "\n",
      "         [[0.0139]]],\n",
      "\n",
      "\n",
      "        [[[0.0122]],\n",
      "\n",
      "         [[0.0120]],\n",
      "\n",
      "         [[0.0135]]],\n",
      "\n",
      "\n",
      "        [[[0.0133]],\n",
      "\n",
      "         [[0.0141]],\n",
      "\n",
      "         [[0.0250]]],\n",
      "\n",
      "\n",
      "        [[[0.0185]],\n",
      "\n",
      "         [[0.0161]],\n",
      "\n",
      "         [[0.0127]]],\n",
      "\n",
      "\n",
      "        [[[0.0200]],\n",
      "\n",
      "         [[0.0128]],\n",
      "\n",
      "         [[0.0125]]],\n",
      "\n",
      "\n",
      "        [[[0.0145]],\n",
      "\n",
      "         [[0.0130]],\n",
      "\n",
      "         [[0.0164]]],\n",
      "\n",
      "\n",
      "        [[[0.0200]],\n",
      "\n",
      "         [[0.0175]],\n",
      "\n",
      "         [[0.0256]]],\n",
      "\n",
      "\n",
      "        [[[0.0135]],\n",
      "\n",
      "         [[0.0156]],\n",
      "\n",
      "         [[0.0164]]],\n",
      "\n",
      "\n",
      "        [[[0.0179]],\n",
      "\n",
      "         [[0.0139]],\n",
      "\n",
      "         [[0.0141]]],\n",
      "\n",
      "\n",
      "        [[[0.0143]],\n",
      "\n",
      "         [[0.0213]],\n",
      "\n",
      "         [[0.0137]]],\n",
      "\n",
      "\n",
      "        [[[0.0204]],\n",
      "\n",
      "         [[0.0152]],\n",
      "\n",
      "         [[0.0250]]],\n",
      "\n",
      "\n",
      "        [[[0.0123]],\n",
      "\n",
      "         [[0.0139]],\n",
      "\n",
      "         [[0.0189]]],\n",
      "\n",
      "\n",
      "        [[[0.0156]],\n",
      "\n",
      "         [[0.0128]],\n",
      "\n",
      "         [[0.0172]]],\n",
      "\n",
      "\n",
      "        [[[0.0120]],\n",
      "\n",
      "         [[0.0147]],\n",
      "\n",
      "         [[0.0133]]],\n",
      "\n",
      "\n",
      "        [[[0.0172]],\n",
      "\n",
      "         [[0.0152]],\n",
      "\n",
      "         [[0.0149]]],\n",
      "\n",
      "\n",
      "        [[[0.0133]],\n",
      "\n",
      "         [[0.0185]],\n",
      "\n",
      "         [[0.0125]]],\n",
      "\n",
      "\n",
      "        [[[0.0135]],\n",
      "\n",
      "         [[0.0130]],\n",
      "\n",
      "         [[0.0179]]],\n",
      "\n",
      "\n",
      "        [[[0.0127]],\n",
      "\n",
      "         [[0.0130]],\n",
      "\n",
      "         [[0.0149]]],\n",
      "\n",
      "\n",
      "        [[[0.0172]],\n",
      "\n",
      "         [[0.0164]],\n",
      "\n",
      "         [[0.0143]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pdb\n",
    "\n",
    "class CustomPoolingLayer(nn.Module):\n",
    "    def __init__(self, r=3, window_size=3):\n",
    "        super(CustomPoolingLayer, self).__init__()\n",
    "        self.r = r\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_scaled = image * 255\n",
    "        windows_horizontal = []\n",
    "        Lr_all_horizontal = []\n",
    "\n",
    "        for i in range(image_scaled.size(2) - self.window_size + 1):\n",
    "            for j in range(image_scaled.size(3) - self.window_size + 1):\n",
    "                window = image_scaled[:, :, i:i+self.window_size, j:j+self.window_size]\n",
    "                windows_horizontal.append(window)\n",
    "\n",
    "                max_pool = nn.MaxPool2d(kernel_size=self.window_size)\n",
    "                max_pool_output = max_pool(window)\n",
    "                min_pool_output = -max_pool(-window)\n",
    "\n",
    "                nr = torch.ceil(max_pool_output / self.r) - torch.ceil(min_pool_output / self.r) - 1\n",
    "                Mr = torch.sum(nr)\n",
    "                Q_mr = nr / (self.window_size - self.r + 1)\n",
    "                L_r = (Mr**2) * Q_mr / (Mr * Q_mr)**2\n",
    "\n",
    "        return L_r\n",
    "\n",
    "# Example usage:\n",
    "# Create an instance of the CustomPoolingLayer\n",
    "custom_pooling_layer = CustomPoolingLayer(r=3, window_size=3)\n",
    "image = torch.rand((128, 3, 13, 13), dtype=torch.float32)\n",
    "# Assuming 'your_image_tensor' is your input tensor\n",
    "result = custom_pooling_layer(image)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3, 3])\n",
      "tensor([[[[0.5000]],\n",
      "\n",
      "         [[0.2000]]],\n",
      "\n",
      "\n",
      "        [[[0.2500]],\n",
      "\n",
      "         [[0.1667]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class CustomPoolingLayer(nn.Module):\n",
    "    def __init__(self, r=3, window_size=3):\n",
    "        super(CustomPoolingLayer, self).__init__()\n",
    "        self.r = r\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, image):\n",
    "        windows_horizontal = image.unfold(2, self.window_size, 1).unfold(3, self.window_size, 1)\n",
    "        windows_horizontal = windows_horizontal.squeeze(0).squeeze(0)\n",
    "        print(windows_horizontal.shape)\n",
    "        max_pool = nn.MaxPool2d(kernel_size=self.window_size)\n",
    "        max_pool_output = max_pool(windows_horizontal)\n",
    "        min_pool_output = -max_pool(-windows_horizontal)\n",
    "\n",
    "        nr = torch.ceil(max_pool_output / self.r) - torch.ceil(min_pool_output / self.r) - 1\n",
    "        Mr = torch.sum(nr)\n",
    "        Q_mr = nr / (self.window_size - self.r + 1)\n",
    "        L_r = (Mr**2) * Q_mr / (Mr * Q_mr)**2\n",
    "\n",
    "        return L_r\n",
    "\n",
    "# Example usage:\n",
    "# Create an instance of the CustomPoolingLayer\n",
    "custom_pooling_layer = CustomPoolingLayer(r=3, window_size=3)\n",
    "\n",
    "# Assuming 'your_image_tensor' is your input tensor\n",
    "image = torch.tensor([[9, 11, 8, 20], [18, 18, 17, 3], [10, 7, 18, 16], [18, 17, 22, 13]], dtype=torch.float32)\n",
    "image = image.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Get the result using the custom pooling layer\n",
    "result = custom_pooling_layer(image)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch dimension stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomPoolingLayer(nn.Module):\n",
    "    def __init__(self, r=3, window_size=3):\n",
    "        super(CustomPoolingLayer, self).__init__()\n",
    "        self.r = r\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, image):\n",
    "        batch_size, channels, image_height, image_width = image.size()\n",
    "\n",
    "        # Initialize variables to accumulate results across batches\n",
    "        all_L_r = []\n",
    "\n",
    "        # Iterate over batches\n",
    "        for batch in range(batch_size):\n",
    "            # Extract the current batch's unfolded window\n",
    "            windows_horizontal = image[batch].unfold(1, self.window_size, 1).unfold(2, self.window_size, 1)\n",
    "            windows_horizontal = windows_horizontal.squeeze(0)\n",
    "\n",
    "            # Perform operations independently for each window in the current batch\n",
    "            max_pool = nn.MaxPool2d(kernel_size=self.window_size)\n",
    "            max_pool_output = max_pool(windows_horizontal)\n",
    "            min_pool_output = -max_pool(-windows_horizontal)\n",
    "\n",
    "            nr = torch.ceil(max_pool_output / self.r) - torch.ceil(min_pool_output / self.r) - 1\n",
    "            Mr = torch.sum(nr)\n",
    "            Q_mr = nr / (self.window_size - self.r + 1)\n",
    "            L_r = (Mr**2) * Q_mr / (Mr * Q_mr)**2\n",
    "\n",
    "            # Accumulate the result for the current batch\n",
    "            all_L_r.append(L_r)\n",
    "\n",
    "        # Concatenate results along the batch dimension\n",
    "        all_L_r = torch.cat(all_L_r, dim=0)\n",
    "\n",
    "        return all_L_r\n",
    "\n",
    "# Example usage:\n",
    "# Create an instance of the CustomPoolingLayer\n",
    "custom_pooling_layer = CustomPoolingLayer(r=3, window_size=3)\n",
    "\n",
    "\n",
    "image = torch.tensor([[9, 11, 8, 20], [18, 18, 17, 3], [10, 7, 18, 16], [18, 17, 22, 13]], dtype=torch.float32)\n",
    "image = image.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Get the result using the custom pooling layer\n",
    "result = custom_pooling_layer(image)\n",
    "print(result.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channel dimension stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomPoolingLayer(nn.Module):\n",
    "    def __init__(self, r=3, window_size=3):\n",
    "        super(CustomPoolingLayer, self).__init__()\n",
    "        self.r = r\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Scale the input tensor to the range [0, 255]\n",
    "        image_scaled = image * 255.0\n",
    "\n",
    "        windows_horizontal = image_scaled.unfold(2, self.window_size, 1).unfold(3, self.window_size, 1)\n",
    "        windows_horizontal = windows_horizontal.squeeze(0).squeeze(0)\n",
    "\n",
    "        max_pool = nn.MaxPool2d(kernel_size=self.window_size)\n",
    "        max_pool_output = max_pool(windows_horizontal)\n",
    "        min_pool_output = -max_pool(-windows_horizontal)\n",
    "\n",
    "\n",
    "        nr = torch.ceil(max_pool_output / self.r) - torch.ceil(min_pool_output / self.r) - 1\n",
    "        Mr = torch.sum(nr)\n",
    "        Q_mr = nr / (self.window_size - self.r + 1)\n",
    "        L_r = (Mr**2) * Q_mr / (Mr * Q_mr)**2\n",
    "\n",
    "        return L_r\n",
    "\n",
    "# Example usage:\n",
    "# Create an instance of the CustomPoolingLayer\n",
    "custom_pooling_layer = CustomPoolingLayer(r=3, window_size=3)\n",
    "\n",
    "# Assuming 'your_image_tensor' is your input tensor with shape (1, 1, 4, 4)\n",
    "image = torch.rand((1, 1, 4, 4), dtype=torch.float32)\n",
    "\n",
    "# Get the result using the custom pooling layer\n",
    "result = custom_pooling_layer(image)\n",
    "print(result.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized dimension stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0208]],\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0169]],\n",
      "\n",
      "         [[0.0137]],\n",
      "\n",
      "         [[0.0159]]],\n",
      "\n",
      "\n",
      "        [[[0.0149]],\n",
      "\n",
      "         [[0.0156]],\n",
      "\n",
      "         [[0.0152]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0169]],\n",
      "\n",
      "         [[0.0137]],\n",
      "\n",
      "         [[0.0141]]],\n",
      "\n",
      "\n",
      "        [[[0.0128]],\n",
      "\n",
      "         [[0.0167]],\n",
      "\n",
      "         [[0.0141]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0172]],\n",
      "\n",
      "         [[0.0185]],\n",
      "\n",
      "         [[0.0154]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0137]],\n",
      "\n",
      "         [[0.0135]],\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         [[0.0123]]],\n",
      "\n",
      "\n",
      "        [[[0.0133]],\n",
      "\n",
      "         [[0.0133]],\n",
      "\n",
      "         [[0.0147]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         [[0.0145]],\n",
      "\n",
      "         [[0.0123]]],\n",
      "\n",
      "\n",
      "        [[[0.0137]],\n",
      "\n",
      "         [[0.0137]],\n",
      "\n",
      "         [[0.0175]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0179]],\n",
      "\n",
      "         [[0.0179]],\n",
      "\n",
      "         [[0.0154]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomPoolingLayer(nn.Module):\n",
    "    def __init__(self, r=3, window_size=3):\n",
    "        super(CustomPoolingLayer, self).__init__()\n",
    "        self.r = r\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_scaled = image * 255\n",
    "        batch_size, channels, image_height, image_width = image.size()\n",
    "\n",
    "        # Initialize variables to accumulate results across batches\n",
    "        all_L_r = []\n",
    "        all_L_r_channel = []\n",
    "\n",
    "        # Iterate over batches\n",
    "        for batch in range(batch_size):\n",
    "            # Iterate over channels\n",
    "            channel_L_r = []\n",
    "            for channel in range(channels):\n",
    "                # Extract the current batch's unfolded window for the current channel\n",
    "                windows_horizontal = image_scaled[batch, channel].unfold(0, self.window_size, 1).unfold(1, self.window_size, 1)\n",
    "                windows_horizontal = windows_horizontal.squeeze(0)\n",
    "\n",
    "                # Perform operations independently for each window in the current channel\n",
    "                max_pool = nn.MaxPool2d(kernel_size=self.window_size)\n",
    "                max_pool_output = max_pool(windows_horizontal)\n",
    "                min_pool_output = -max_pool(-windows_horizontal)\n",
    "\n",
    "                nr = torch.ceil(max_pool_output / self.r) - torch.ceil(min_pool_output / self.r) - 1\n",
    "                Mr = torch.sum(nr)\n",
    "                Q_mr = nr / (self.window_size - self.r + 1)\n",
    "                L_r = (Mr**2) * Q_mr / (Mr * Q_mr)**2\n",
    "                # Accumulate the result for the current channel\n",
    "                channel_L_r.append(L_r)\n",
    "\n",
    "            # Concatenate results along the channel dimension for the current batch\n",
    "            channel_L_r = torch.cat(channel_L_r, dim=0)\n",
    "            all_L_r_channel.append(channel_L_r)\n",
    "\n",
    "        # Concatenate results along the batch dimension\n",
    "        all_L_r_batch = torch.cat(all_L_r_channel, dim=1)\n",
    "\n",
    "        return all_L_r_batch\n",
    "\n",
    "# Example usage:\n",
    "# Create an instance of the CustomPoolingLayer\n",
    "custom_pooling_layer = CustomPoolingLayer(r=3, window_size=3)\n",
    "\n",
    "# Assuming 'your_image_tensor' is your input tensor with shape [128, 1, 13, 13] (128 batches, 1 channel, 13x13 image)\n",
    "image = torch.rand((128, 3, 13, 13), dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Get the result using the custom pooling layer\n",
    "result = custom_pooling_layer(image)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3, 3])\n",
      "torch.Size([1, 2, 1, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "class CustomPoolingLayer(nn.Module):\n",
    "    def __init__(self, r=3, window_size=3):\n",
    "        super(CustomPoolingLayer, self).__init__()\n",
    "        self.r = r\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_scaled = image * 255\n",
    "        batch_size, channels, image_height, image_width = image.size()\n",
    "\n",
    "        # Initialize variables to accumulate results across batches\n",
    "        all_L_r = []\n",
    "\n",
    "        # Iterate over batches\n",
    "        for batch in range(batch_size):\n",
    "            # Iterate over channels\n",
    "            channel_L_r = []\n",
    "            for channel in range(channels):\n",
    "                # Extract the current batch's unfolded window for the current channel\n",
    "                windows_horizontal = image_scaled[batch, channel].unfold(0, self.window_size, 1).unfold(1, self.window_size, 1)\n",
    "                windows_horizontal = windows_horizontal.squeeze(0)\n",
    "\n",
    "                # Perform operations independently for each window in the current channel\n",
    "                max_pool = nn.MaxPool2d(kernel_size=self.window_size)\n",
    "                max_pool_output = max_pool(windows_horizontal)\n",
    "                min_pool_output = -max_pool(-windows_horizontal)\n",
    "\n",
    "                nr = torch.ceil(max_pool_output / self.r) - torch.ceil(min_pool_output / self.r) - 1\n",
    "                Mr = torch.sum(nr)\n",
    "                Q_mr = nr / (self.window_size - self.r + 1)\n",
    "                L_r = (Mr**2) * Q_mr / (Mr * Q_mr)**2\n",
    "\n",
    "                # Accumulate the result for the current channel\n",
    "                channel_L_r.append(L_r)\n",
    "\n",
    "            # Stack results across channels for the current batch\n",
    "            channel_L_r = torch.stack(channel_L_r, dim = 1)\n",
    "            all_L_r.append(channel_L_r)\n",
    "\n",
    "        # Stack results across batches\n",
    "        all_L_r = torch.stack(all_L_r)\n",
    "\n",
    "        return all_L_r\n",
    "    \n",
    "\n",
    "# Create an instance of the CustomPoolingLayer\n",
    "custom_pooling_layer = CustomPoolingLayer(r=3, window_size=3)\n",
    "\n",
    "# Assuming 'your_image_tensor' is your input tensor with shape [128, 1, 13, 13] (128 batches, 1 channel, 13x13 image)\n",
    "image = torch.rand((1, 1, 4, 4), dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Get the result using the custom pooling layer\n",
    "result = custom_pooling_layer(image)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0.0159]],\n",
      "\n",
      "         [[0.0125]]],\n",
      "\n",
      "\n",
      "        [[[0.0125]],\n",
      "\n",
      "         [[0.0125]]]])]\n",
      "[tensor([[[[0.0159]],\n",
      "\n",
      "         [[0.0125]]],\n",
      "\n",
      "\n",
      "        [[[0.0125]],\n",
      "\n",
      "         [[0.0125]]]]), tensor([[[[0.0132]],\n",
      "\n",
      "         [[0.0132]]],\n",
      "\n",
      "\n",
      "        [[[0.0128]],\n",
      "\n",
      "         [[0.0128]]]])]\n",
      "[tensor([[[[0.0159]],\n",
      "\n",
      "         [[0.0125]]],\n",
      "\n",
      "\n",
      "        [[[0.0125]],\n",
      "\n",
      "         [[0.0125]]]]), tensor([[[[0.0132]],\n",
      "\n",
      "         [[0.0132]]],\n",
      "\n",
      "\n",
      "        [[[0.0128]],\n",
      "\n",
      "         [[0.0128]]]]), tensor([[[[0.0127]],\n",
      "\n",
      "         [[0.0154]]],\n",
      "\n",
      "\n",
      "        [[[0.0135]],\n",
      "\n",
      "         [[0.0154]]]])]\n"
     ]
    }
   ],
   "source": [
    "class CustomPoolingLayer(nn.Module):\n",
    "    def __init__(self, r=3, window_size=3):\n",
    "        super(CustomPoolingLayer, self).__init__()\n",
    "        self.r = r\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_scaled = image * 255\n",
    "        batch_size, channels, image_height, image_width = image.size()\n",
    "\n",
    "        # Initialize variables to accumulate results across batches\n",
    "        all_L_r = []\n",
    "\n",
    "        # Iterate over batches\n",
    "        for batch in range(batch_size):\n",
    "            # Iterate over channels\n",
    "            channel_L_r = []\n",
    "            for channel in range(channels):\n",
    "                # Extract the current batch's unfolded window for the current channel\n",
    "                windows_horizontal = image_scaled[batch, channel].unfold(0, self.window_size, 1).unfold(1, self.window_size, 1)\n",
    "\n",
    "                # Perform operations independently for each window in the current channel\n",
    "                max_pool = nn.MaxPool2d(kernel_size=self.window_size)\n",
    "                max_pool_output = max_pool(windows_horizontal)\n",
    "                min_pool_output = -max_pool(-windows_horizontal)\n",
    "\n",
    "                nr = torch.ceil(max_pool_output / self.r) - torch.ceil(min_pool_output / self.r) - 1\n",
    "                Mr = torch.sum(nr)\n",
    "                Q_mr = nr / (self.window_size - self.r + 1)\n",
    "                L_r = (Mr**2) * Q_mr / (Mr * Q_mr)**2\n",
    "                windows_horizontal[1:2, 1:2, :, :] = L_r\n",
    "\n",
    "                # Accumulate the result for the current channel\n",
    "                channel_L_r.append(L_r)\n",
    "                print(channel_L_r)\n",
    "\n",
    "            # Stack results along the channel dimension\n",
    "            channel_L_r = torch.stack(channel_L_r, dim=1)\n",
    "            all_L_r.append(channel_L_r)\n",
    "\n",
    "        # Stack results along the batch dimension\n",
    "        all_L_r = torch.stack(all_L_r, dim=0)\n",
    "\n",
    "        return all_L_r\n",
    "\n",
    "# Example usage\n",
    "custom_pooling = CustomPoolingLayer(r=3, window_size=3)\n",
    "input_data = torch.rand(1, 3, 4, 4)\n",
    "output = custom_pooling(input_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
